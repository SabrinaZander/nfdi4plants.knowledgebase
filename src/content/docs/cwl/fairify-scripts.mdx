---
title: From Script to CWL
lastUpdated: 2025-07-09
authors:
  - dominik-brilhaus
---

import { FileTree } from '@astrojs/starlight/components';
import { Steps } from '@astrojs/starlight/components';
import { Tabs, TabItem } from '@astrojs/starlight/components';
import Mermaid from '@components/mdx/Mermaid.astro'


This starterâ€™s guide to **FAIRifying script-based data analysis workflows** walks you through the steps to turn your existing script into a **reusable CWL workflow**. Whether you use Bash, Python, R, or another language the same logic applies.

## Recommendations for FAIR data analysis

These recommendations are good coding practice independently of the steps below.

<Steps>

1. Follow the [KISS principle](https://en.wikipedia.org/wiki/KISS_principle) and **Keep It Single-Step**
    - A script should ideally only execute a single process
    - Modular, easier to combine in multi-step pipelines
    - More reusable
    - Easier to follow and annotate what the script consumes (`inputs`) and creates (`outputs`)
2. **Do not hard code input or output paths**
    - Reusability
3. Collect information about external packages and tools the script depends on
    - Packages
    - Record the version
    - Record online resources (URLs, biotool ro SciCrunch IDs, GitHub site)

</Steps>

---

## Identify Inputs and Outputs

Look through your script and **identify what data it reads and what data it writes**. These are your `inputs` and `outputs`.

### Examples

- Input: a CSV file, a FASTA file.
- Output: a figure, a table, a processed file.

### Goal

Make a list like:

```yaml
inputs:
  - input_data: "data.csv"

outputs:
  - result_file: "results/summary.csv"
```

---

## Refactor the script

**Replace hard-coded paths** (pointing to your `inputs` and `outputs`) with command-line arguments. For easier handling, move these input/output variables to its own section (e.g. on top of your script).

### Example

Change this...

```python  title="Old"
data = pd.read_csv("data.csv")
```

...to this:

```python title="New"
import sys
data = pd.read_csv(sys.argv[1])
```

:::tip
Use `argparse` in Python or `commandArgs()` in R for clarity.
:::

### Goal

This makes your script **modular and reusable**.

---

## Write a minimal Workflow CWL File

Create a file named `workflow.cwl`.

### Example

```yaml title="workflow.cwl"
cwlVersion: v1.2
class: CommandLineTool
baseCommand: python3
inputs:
  script_file:
    type: File
    inputBinding:
      position: 1
  input_file:
    type: File
    inputBinding:
      position: 2
outputs:
  output_file:
    type: File
    outputBinding:
      glob: "output.csv"
requirements: []
```

### Goal 

This wraps your script in CWL as a `CommandLineTool`.
It basically runs `python3 script.py output.csv`. 

:::note
Make sure your script is saved alongside `workflow.cwl`.
:::

---

## Add a minimal `Run`

Create a `run.cwl` file that uses your `workflow.cwl`.

### Example

```yaml titles="run.cwl"
cwlVersion: v1.2
class: Workflow

inputs:
  input_file: File

steps:
  step1:
    run: workflow.cwl
    in:
      script_file: { default: script.py }
      input_file: input_file
    out: [output_file]

outputs:
  output_file:
    type: File
    outputSource: step1/output_file
```

Also create a suitable `run.yml` to provide the parameters required by the `run.cwl`:

```yaml title="run.yml"
input_file: data.csv
```

### Goal


Then run it with:

```bash
cwltool run.cwl run.yml
```

---

## Identify Required Tools and Packages

Go back to your script and **list all external packages and tools it depends on**.

### Example

Look for e.g.

- Python packages (`pandas`, `numpy`)
- R libraries (`ggplot2`)
- Command-line tools (`samtools`, `awk`)

### Goal

---

## Collect Metadata About Dependencies

Record:

- **Exact package names**
- **Versions**

### Example

- pandas, version 1.5.3
- numpy, version 1.23.0

This helps others reproduce your analysis precisely.

## Add dependencies to `hints` and `requirements`

In `workflow.cwl`, add a `SoftwareRequirement`

### Soft requirements = `hints`

Specify software and resource requirements under `hints`

- add `SoftwareRequirement` to specify software version and reference
  - `package: ` name of the software or package
  - `specs: ` reference url from https://identifiers.org/biotools/ or SciCrunch https://identifiers.org/rrid/
  - `version: [ "0.11.9" ]`

- add `ResourceRequirement` to specify the required compute resources

### Example

```yaml
hints:
  - class: SoftwareRequirement
    packages:
      - package: python
        version: [3.10]
      - package: pandas
        version: [1.5.3]
```

### Hard requirements = `requirements`

Use the `requirements` primarily to specify hard requirements needed to run the current `CommandLineTool` or `Workflow` document

---

## Add a Docker Container

For full portability, specify a container with all dependencies.
Use the `DockerRequirement` to load a published Docker image or reference a local `Dockerfile`.

### Example

Load a public image

```yaml
requirements:
  - class: DockerRequirement
    dockerPull: python:3.10-slim
```

Create your own `Dockerfile` and build an image:

```Dockerfile
FROM python:3.10-slim
RUN pip install pandas==1.5.3 numpy==1.23.0
```

## Metadata

### Namespaces and schemas

Adding namespaces and schemas allows to reuse them elsewhere in a CWL document

```yaml
$namespaces:
  s: https://schema.org/
  edam: http://edamontology.org/

$schemas:
  - https://schema.org/version/latest/schemaorg-current-https.rdf
  - http://edamontology.org/EDAM_1.18.owl
```

### Attribute authors and contributors

```yaml
s:author:
  - class: s:Person
    s:identifier: <author ORCID>
    s:email: mailto:<author email>
    s:name: <author name>

s:contributor:
  - class: s:Person
    s:identifier: <contributor ORCID>
    s:email: mailto:<contributor email>
    s:name: <contributor name>

```


## Resources

This guide is adapted and includes recommendations from:

- https://www.commonwl.org/user_guide/topics/best-practices.html
- https://www.commonwl.org/user_guide/topics/metadata-and-authorship.html 