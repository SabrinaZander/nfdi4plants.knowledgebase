---
title: From Script to CWL
lastUpdated: 2025-07-10
authors:
  - dominik-brilhaus
---

import { FileTree } from '@astrojs/starlight/components';
import { Steps } from '@astrojs/starlight/components';
import { Tabs, TabItem, Card } from '@astrojs/starlight/components';
import Mermaid from '@components/mdx/Mermaid.astro'

This starter’s guide to **FAIRifying script-based data analysis workflows** walks you through the steps to turn your existing script into a **reusable CWL workflow**. Whether you use Python, R, Bash, or another language the same logic applies.

## Recommendations for FAIR data analysis

The following considerations are recommended as FAIR coding practice independently of the steps below. 
The goal is to be able to share the data analysis with as much context and functionality as possible to replicate, reproduce or reuse it in other contexts and on other data of the same type.

<Steps>

1. Follow the [KISS principle](https://en.wikipedia.org/wiki/KISS_principle) and **Keep It Single-Step**
    - A script should ideally only execute a single process. Avoid mixing too many, independent processes in a single script. 
    - Focussing on a clear task increases the script's reusabaility in other contexts and facilitates the comprehension and annotation of what the script achieves, what `inputs` it consumes and which `outputs` it creates
2. Collect information about external packages and tools the script depends on
    - Record the packages, libraries, tools
    - Record their version
    - Record available resources (URLs, biotool ro SciCrunch IDs, GitHub site)
2. Separate the process from the data
    - Modular, easier to combine in multi-step pipelines
    - **Do not hard code input or output paths**
    - Reusability

</Steps>


## Refactor the script

Look through your script and identify **what data it reads and what data it writes**. These are your `inputs` and `outputs`.

  - Input: path to a FASTA file, a CSV file (e.g. `data.csv`)
  - Output: path to where a result is written, e.g. a figure, a processed file, a table (`sorted.csv`)

<Steps>
  1. For easier overview and handling, move the input and output variables to its own section (e.g. on top of your script).
  1. Replace any paths pointing outside the ARC
  2. Replace **absolute** with **relative** paths
</Steps>

:::tip[Goal]
Reusability – Collaborators receive the ARC and its contents and cannot access referenced data outside the ARC.
:::

## From interactive to reusable

<Steps>
  1. **Replace hard-coded paths** (pointing to your `inputs` and `outputs`) with command-line arguments.
</Steps>


<Card icon="pen" title="Example">

Each script below reads a CSV file (`data.csv`), sorts it by the first column, and writes the result to `sorted.csv`.

Change this...

<Tabs syncKey="pl">

<TabItem label="Python" icon='seti:python'>

```python 
import sys
import pandas as pd

data = pd.read_csv("data.csv")
data_sorted = data.sort_values(by=data.columns[0])
data_sorted.to_csv("sorted.csv", index=False)
```

</TabItem>

<TabItem label="R" icon='seti:R'>

```r 
data <- read.csv("data.csv")
data_sorted <- data[order(data[[1]]), ]
write.csv(data_sorted, "sorted.csv", row.names=FALSE)
```

</TabItem>


<TabItem label="Bash" icon='seti:shell'>

```bash 
#!/bin/bash

(head -n 1 data.csv && tail -n +2 data.csv | sort) > sorted.csv
```

</TabItem>

</Tabs>

...to this:

<Tabs syncKey="pl">

<TabItem label="Python" icon='seti:python'>

```python title="sort-csv.py"
import sys
import pandas as pd

input_file = sys.argv[1]
output_file = sys.argv[2]

data = pd.read_csv(input_file)
data_sorted = data.sort_values(by=data.columns[0])
data_sorted.to_csv(output_file, index=False)
```

</TabItem>

<TabItem label="R" icon='seti:R'>

```r title="sort-csv.R"
args <- commandArgs(trailingOnly=TRUE)
input_file <- args[1]
output_file <- args[2]

data <- read.csv(input_file)
data_sorted <- data[order(data[[1]]), ]
write.csv(data_sorted, output_file, row.names=FALSE)
```

</TabItem>

<TabItem label="Bash" icon='seti:shell'>

```bash title="sort-csv.sh"
#!/bin/bash
input_file="$1"
output_file="$2"

(head -n 1 "$input_file" && tail -n +2 "$input_file" | sort) > "$output_file"
```

</TabItem>

</Tabs>


The script can then be run via

<Tabs syncKey="pl">

<TabItem label="Python" icon='seti:python'>

```bash
python sort-csv.py data.csv sorted.csv
```

</TabItem>

<TabItem label="R" icon='seti:R'>

```bash
Rscript sort-csv.R data.csv sorted.csv
```

</TabItem>

<TabItem label="Bash" icon='seti:shell'>

```bash
bash sort-csv.sh data.csv sorted.csv
```

</TabItem>

</Tabs>

</Card>

:::tip[Goal]
Reusability – This makes your script more **modular and reusable** on other data inputs.
:::

## Write a minimal workflow CWL descriptor

Once the interactive script is converted into a generally executable script, it can be described with a CWL descriptor file, that wraps the script as a `CommandLineTool`.

<Card icon="pen" title="Example">

<Tabs syncKey="pl">

  <TabItem label="Python" icon='seti:python'>

    <Steps>
    1. Create a file named `workflow.cwl`    

        ```yaml title="workflow.cwl"
        cwlVersion: v1.2
        class: CommandLineTool
        requirements:
          - class: InitialWorkDirRequirement
            listing:
              - entryname: sort-csv.py
                entry:
                  $include: sort-csv.py
        baseCommand: [python3, sort-csv.py]
        inputs:
          input_file:
            type: File
            inputBinding:
              position: 1
          output_filename:
            type: string
            inputBinding:
              position: 2
        outputs:
          output_file:
            type: File
            outputBinding:
              glob: $(inputs.output_filename)
        ```

        This basically runs `python3 sort-csv.py data.csv sorted.csv`.

    2. Save it alongside the `sort-csv.py` script.
    3. Store the workflow.cwl and script in an ARC's `workflows` folder

       <FileTree>
        - ...
        - workflows
          - sort-csv
            - sort-csv.py
            - workflow.cwl
        - runs
          - ...
        - ...
        </FileTree>
    
    </Steps>

    </TabItem>

    <TabItem label="R" icon='seti:R'>

    <Steps>
    1. Create a file named `workflow.cwl`

        ```yaml title="workflow.cwl"
        cwlVersion: v1.2
        class: CommandLineTool
        requirements:
          - class: InitialWorkDirRequirement
            listing:
              - entryname: sort-csv.R
                entry:
                  $include: sort-csv.R
        baseCommand: [Rscript, sort-csv.R]
        inputs:
          input_file:
            type: File
            inputBinding:
              position: 1
          output_filename:
            type: string
            inputBinding:
              position: 2
        outputs:
          output_file:
            type: File
            outputBinding:
              glob: $(inputs.output_filename)
        ```

        This basically runs `Rscript sort-csv.R data.csv sorted.csv`.

    2. Save it alongside the `sort-csv.R` script.
    3. Store the workflow.cwl and script in an ARC's `workflows` folder

       <FileTree>
        - ...
        - workflows
          - sort-csv
            - sort-csv.R
            - workflow.cwl
        - runs
          - ...
        - ...
        </FileTree>
    
    
    </Steps>

    </TabItem>

    <TabItem label="Bash" icon='seti:shell'>

    <Steps>

    1. Create a file named `workflow.cwl`
    
        ```yaml title="workflow.cwl"
        cwlVersion: v1.2
        class: CommandLineTool
        requirements:
          - class: InitialWorkDirRequirement
            listing:
              - entryname: sort-csv.sh
                entry:
                  $include: sort-csv.sh
        baseCommand: [bash, sort-csv.sh]
        inputs:
          input_file:
            type: File
            inputBinding:
              position: 1
          output_filename:
            type: string
            inputBinding:
              position: 2
        outputs:
          output_file:
            type: File
            outputBinding:
              glob: $(inputs.output_filename)
        ```

        This basically runs `bash sort-csv.sh data.csv sorted.csv`.

    2. Save it alongside the `sort-csv.sh` script.
    3. Store the workflow.cwl and script in an ARC's `workflows` folder

       <FileTree>
        - ...
        - workflows
          - sort-csv
            - sort-csv.sh
            - workflow.cwl
        - runs
          - ...
        - ...
        </FileTree>
    </Steps>

    </TabItem>

</Tabs>

</Card>

:::tip[Goal]
Interoperability – Together with the `workflow.cwl`, the script is now a "workflow". As such, it can become a step in a pipeline.
:::

## Add a minimal `Run`

<Card icon="pen" title="Example">

<Steps>

  1. Create a `run.cwl` file that uses the `workflow.cwl`.

      ```yaml title="run.cwl" { 10 }
      cwlVersion: v1.2
      class: Workflow

      inputs:
        input_file: File
        output_filename: string

      steps:
        step1:
          run: ../../workflows/sort-csv/workflow.cwl
          in:
            input_file: input_file
            output_filename: output_filename
          out: [output_file]

      outputs:
        output_file:
          type: File
          outputSource: step1/output_file
      ```

      :::note[Workflow reference]
      the `../../workflows/sort-csv/workflow.cwl`
      :::

  2. Create a `run.yml` to provide the parameters required by the `run.cwl`:

      ```yaml title="run.yml"
      input_file:
        class: File
        path: data.csv
      output_filename: sorted.csv
      ```

  3. Place both files in an ARC's `runs` folder

      <FileTree>
        - ...
        - workflows
          - sort-csv
            - sort-csv.py / .R / .sh
            - workflow.cwl
        - runs
          - sort-my-data-table
            - **run.cwl**
            - **run.yml**
        - ...
      </FileTree>
      

  </Steps>

</Card>

The workflow can now be executed with

```bash
cwltool run.cwl run.yml
```

:::tip[Goal]
Interoperability – separate generic process from concrete data
:::


## Identify Required Tools and Packages

Go back to your script and **list all external packages and tools it depends on**.

Look for e.g.

- Python packages (`pandas`, `numpy`)
- R libraries (`ggplot2`)
- Command-line tools (`samtools`, `awk`)

:::tip[Goal]
- Reuse
- Find
- Access 
:::

## Collect Metadata About Dependencies

Record:
- **Exact package names**
- **Versions**

- pandas, version 1.5.3
- numpy, version 1.23.0

:::tip[Goal]
Reusability – This helps others reproduce your analysis precisely.
:::

## Add dependencies to `hints` and `requirements`

In `workflow.cwl`, add a `SoftwareRequirement`

- Soft requirements = `hints`

Specify software and resource requirements under `hints`

- add `SoftwareRequirement` to specify software version and reference
  - `package: ` name of the software or package
  - `specs: ` reference url from https://identifiers.org/biotools/ or SciCrunch https://identifiers.org/rrid/
  - `version: [ "0.11.9" ]`

- add `ResourceRequirement` to specify the required compute resources


- Hard requirements = `requirements`

Use the `requirements` primarily to specify hard requirements needed to run the current `CommandLineTool` or `Workflow` document

<Card icon="pen" title="Example">

```yaml title="workflow.cwl"
...
hints:
  - class: SoftwareRequirement
    packages:
      - package: python
        version: [3.10]
      - package: pandas
        version: [1.5.3]
...
```

</Card>

## Add a container

For full portability, specify a container with all dependencies.
Use the `DockerRequirement` to load a published Docker image or reference a local `Dockerfile`.

<Card icon="pen" title="Example">

Load a public image

```yaml title="workflow.cwl"
...
requirements:
  - class: DockerRequirement
    dockerPull: python:3.10-slim
...
```

:::note[Public containers]

- dockerhub
- biocontainers
- mutli-containers

:::


</Card>

<Card icon="pen" title="Example">

If you cannot find a suitable container matching your dependencies, you can also design a `Dockerfile`. 

<Steps>

1. Create your own `Dockerfile`

    ```dockerfile title="Dockerfile"
    FROM python:3.10-slim
    RUN pip install pandas==1.5.3
    ```

2. Load the `Dockerfile` in `workflow.cwl`

    ```yaml title="workflow.cwl"
    ...
    hints:
      DockerRequirement:
        dockerImageId: "mydocker"
        dockerFile: {$include: "Dockerfile"}
    ...
    ```

</Steps>

</Card>

:::tip[Goal]
Reusability – makes your data analysis workflow portable and reusable
:::


## Metadata

### Namespaces and schemas

Adding namespaces and schemas allows to reuse them elsewhere in a CWL document

```yaml
$namespaces:
  s: https://schema.org/
  edam: http://edamontology.org/

$schemas:
  - https://schema.org/version/latest/schemaorg-current-https.rdf
  - http://edamontology.org/EDAM_1.18.owl
```

### Attribute authors and contributors

```yaml
s:author:
  - class: s:Person
    s:identifier: <author ORCID>
    s:email: mailto:<author email>
    s:name: <author name>

s:contributor:
  - class: s:Person
    s:identifier: <contributor ORCID>
    s:email: mailto:<contributor email>
    s:name: <contributor name>

```


## Resources

This guide is adapted and includes recommendations from:

- https://www.commonwl.org/user_guide/topics/best-practices.html
- https://www.commonwl.org/user_guide/topics/metadata-and-authorship.html 